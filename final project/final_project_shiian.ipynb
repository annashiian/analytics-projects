{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Task Retention\n",
    "df_auth = pd.read_csv('~/shared/problem1-auth_data.csv', sep=';')\n",
    "df_reg = pd.read_csv('~/shared/problem1-reg_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601013, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define, what dataframe is the biggest to use the correct join and not to loose a part of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the definition, which calculate retention. the number of periods is stored in a variable, \n",
    "# to change it anytime.\n",
    "def count_retention(df_auth, df_reg, count_periods=20):\n",
    "    from datetime import timedelta\n",
    "    df = df_auth\\\n",
    "        .merge(df_reg, on='uid', how='left')\n",
    "    df['date_reg'] = pd.to_datetime(df.reg_ts, unit='s')\n",
    "    # from max date subtract periods quantity, required by task\n",
    "    min_date = df.date_reg.max() - timedelta(days=count_periods)\n",
    "    # take only last count_periods of dates\n",
    "    df = df[df.date_reg > min_date]\n",
    "    df['date_auth'] = pd.to_datetime(df.auth_ts, unit='s').dt.strftime('%Y-%m-%d')\n",
    "    df['date_reg'] = df.date_reg.dt.strftime('%Y-%m-%d')\n",
    "    cohorts = df.groupby(['date_reg', 'date_auth'])\\\n",
    "        .agg({'uid': 'nunique'})\\\n",
    "        .sort_values(['date_reg', 'date_auth'])\n",
    "    \n",
    "    # define the function, which приводит дату(строка) к datetime\n",
    "    def parse_date(x): \n",
    "        return pd.to_datetime(x, format=\"%Y-%m-%d\")\n",
    "\n",
    "    # define the function, which calculate a different between dates\n",
    "    def month_diff(a, b):\n",
    "        a = parse_date(a)\n",
    "        b = parse_date(b)\n",
    "        return 12 * 30 * (a.year - b.year) + 30 * (a.month - b.month) + (a.day - b.day)\n",
    "\n",
    "    # define the function, which calculate the difference for each cohort from the start of cohort(first month of purchase)\n",
    "    # to (each)purchase month\n",
    "    def cohort_period(df):\n",
    "        df['CohortPeriod'] = month_diff(df.index.get_level_values('date_auth'), df.index.get_level_values('date_reg'))\n",
    "        df['date'] = df['uid'] / df['uid'][0]\n",
    "        return df\n",
    "    cohorts_def = cohorts.groupby(level=0).apply(cohort_period)\n",
    "    # create a table and return it\n",
    "    return cohorts_def[cohorts_def.CohortPeriod <= count_periods]\\\n",
    "        .reset_index().pivot_table(index='date_reg', columns='CohortPeriod', values='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>CohortPeriod</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_reg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.039116</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>0.049320</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>0.037415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>0.038295</td>\n",
       "      <td>0.045090</td>\n",
       "      <td>0.053119</td>\n",
       "      <td>0.064855</td>\n",
       "      <td>0.069796</td>\n",
       "      <td>0.063002</td>\n",
       "      <td>0.044472</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022181</td>\n",
       "      <td>0.040665</td>\n",
       "      <td>0.045595</td>\n",
       "      <td>0.060382</td>\n",
       "      <td>0.056069</td>\n",
       "      <td>0.069008</td>\n",
       "      <td>0.056685</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019077</td>\n",
       "      <td>0.042462</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>0.067692</td>\n",
       "      <td>0.069538</td>\n",
       "      <td>0.033846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.042998</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.054668</td>\n",
       "      <td>0.062654</td>\n",
       "      <td>0.042383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019018</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.052147</td>\n",
       "      <td>0.037423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018360</td>\n",
       "      <td>0.035496</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024450</td>\n",
       "      <td>0.043399</td>\n",
       "      <td>0.023839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "CohortPeriod   0         1         2         3         4         5         6   \\\n",
       "date_reg                                                                        \n",
       "2020-09-13    1.0  0.008503  0.039116  0.045918  0.051020  0.066327  0.078231   \n",
       "2020-09-14    1.0  0.022236  0.038295  0.045090  0.053119  0.064855  0.069796   \n",
       "2020-09-15    1.0  0.022181  0.040665  0.045595  0.060382  0.056069  0.069008   \n",
       "2020-09-16    1.0  0.019077  0.042462  0.046154  0.046769  0.067692  0.069538   \n",
       "2020-09-17    1.0  0.016585  0.042998  0.045455  0.054668  0.062654  0.042383   \n",
       "2020-09-18    1.0  0.019018  0.045399  0.042331  0.052147  0.037423       NaN   \n",
       "2020-09-19    1.0  0.018360  0.035496  0.047124  0.034884       NaN       NaN   \n",
       "2020-09-20    1.0  0.024450  0.043399  0.023839       NaN       NaN       NaN   \n",
       "2020-09-21    1.0  0.018926  0.029915       NaN       NaN       NaN       NaN   \n",
       "2020-09-22    1.0  0.008531       NaN       NaN       NaN       NaN       NaN   \n",
       "2020-09-23    1.0       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "CohortPeriod        7         8         9         10  \n",
       "date_reg                                              \n",
       "2020-09-13    0.045918  0.049320  0.037415  0.037415  \n",
       "2020-09-14    0.063002  0.044472  0.029030       NaN  \n",
       "2020-09-15    0.056685  0.024646       NaN       NaN  \n",
       "2020-09-16    0.033846       NaN       NaN       NaN  \n",
       "2020-09-17         NaN       NaN       NaN       NaN  \n",
       "2020-09-18         NaN       NaN       NaN       NaN  \n",
       "2020-09-19         NaN       NaN       NaN       NaN  \n",
       "2020-09-20         NaN       NaN       NaN       NaN  \n",
       "2020-09-21         NaN       NaN       NaN       NaN  \n",
       "2020-09-22         NaN       NaN       NaN       NaN  \n",
       "2020-09-23         NaN       NaN       NaN       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run example\n",
    "count_retention(df_auth, df_reg, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 task \n",
    "url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "link = 'https://disk.yandex.ru/d/12gk_ATEeBZm4A'\n",
    "\n",
    "response = requests.get(url + urlencode({'public_key': link}))\n",
    "\n",
    "file_link = json.loads(response.text)['href']\n",
    "df_2 = pd.read_csv(file_link, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# slit test and control group and calculate ARPU\n",
    "df_a = df_2.query(\"testgroup=='a'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARPU_a = df_a.revenue.sum() / df_a.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df_2.query(\"testgroup=='b'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARPU_b = df_b.revenue.sum() / df_b.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.413719736965806"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARPU_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.75128659327863"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARPU_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARPU in a test group higher, then in control.Prove, these difference are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 step : need to check, that data is normally distributed in each group.\n",
    "# so we we apply the Shapiro-Wilk criterion. \n",
    "# as a null hypothesis we take, that distribution is normal. alternative is that distribution is different that normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample = df_a.revenue.sample(1000, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sample = df_b.revenue.sample(1000, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.06614720821380615, pvalue=0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.shapiro(a_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.06715953350067139, pvalue=0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.shapiro(b_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in both groups the distrivution is not normal because p value =0.0\n",
    "# we reject the null hypothesis about the normality of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 since the distribution is not normak, we we apply Mann-Whitney U-test(non-parmetric analogue of the t-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a null hypothesis we take, that there is no difference between average in the general population (ARPU_a=ARPU_b)\n",
    "# as an alternative we take, that the averages in the population are not equal,\n",
    "# that means (ARPU_b is really higher,then ARPU_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=20491259376.0, pvalue=0.06269701316074398)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mannwhitneyu(df_a.revenue, df_b.revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as p-value is approximately 0.06, we cannot reject the null hypothesis. \n",
    "# that means that there is no statistically differenceт between ARPU_a and ARPU_b\n",
    "# and that means, that ARPU mean is not statistically significantly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 task\n",
    "1.What metrics can be used to evaluate the results of the last past event?\n",
    "\n",
    "-At first, I would calculate retention to make sure that people return to the game more often\n",
    "-I will also calculate DAU to see the impact on the number of players on any given day\n",
    "-I will calculate the CSAT customer satisfaction score in order to understand that people like the event\n",
    "-It will also be useful to know the ASL average session length to look at the involvement in the game\n",
    "\n",
    "We proceed from the hypothesis that players who spend more time in the game and enter the game more often\n",
    "more involved in the gameplay - more likely to make an in-game purchase\n",
    "\n",
    "2. Suppose, that in another event, we have complicated the mechanics of events so that for each unsuccessful attempt\n",
    "completing a level, the player will roll back several levels. Will the set of performance evaluation metrics change?\n",
    "If so, how?\n",
    "\n",
    "We need to make sure players don't play worse because of a tougher event.\n",
    "\n",
    "-For example, pay attention to the DAU, do not become less players every day?!\n",
    "-You also need to look at how the average session duration has changed. It could both decrease when the player quits after several failed attempts, and increase when the player spends more time trying to complete the level \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
